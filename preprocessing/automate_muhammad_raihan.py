# -*- coding: utf-8 -*-
"""automate_Muhammad Raihan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IfMIomuTfE12C8PFPiWuLBHtO-gAouQc
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import joblib
import os
import warnings
warnings.filterwarnings('ignore')

class BookPopularityPreprocessor:
    """
    Kelas untuk melakukan preprocessing otomatis pada dataset buku
    """

    def __init__(self, missing_threshold=0.5, test_size=0.2, random_state=42):
        """
        Parameters:
        -----------
        missing_threshold : float
            Threshold untuk drop kolom dengan missing values (0-1)
        test_size : float
            Proporsi data untuk testing (0-1)
        random_state : int
            Random state untuk reproducibility
        """
        self.missing_threshold = missing_threshold
        self.test_size = test_size
        self.random_state = random_state
        self.label_encoders = {}
        self.scaler = StandardScaler()
        self.feature_columns = []

    def load_data(self, filepath):
        """Load dataset dari file CSV"""
        # Corrected URL: Replaced spaces with %20
        url = "https://raw.githubusercontent.com/mraihanbaa-cpu/Eksperimen_SML_Muhammad-Raihan/main/top%201000%20most%20swapped%20books_raw.csv"
        df = pd.read_csv(url)
        print(f"✓ Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns")
        return df

    def handle_missing_values(self, df):
        """Handle missing values: drop columns dan imputasi"""
        print("\n--- Handling Missing Values ---")
        df = df.copy()

        # Drop kolom dengan missing >threshold
        missing_ratio = df.isnull().sum() / len(df)
        cols_to_drop = missing_ratio[missing_ratio > self.missing_threshold].index.tolist()
        if cols_to_drop:
            print(f"✓ Dropping columns with >{self.missing_threshold*100}% missing: {cols_to_drop}")
            df = df.drop(columns=cols_to_drop)

        # Imputasi numerik dengan median
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if df[col].isnull().sum() > 0:
                median_value = df[col].median()
                df[col].fillna(median_value, inplace=True)
                print(f"✓ {col}: filled with median ({median_value:.2f})")

        # Imputasi kategorikal dengan mode
        categorical_cols = df.select_dtypes(include=['object', 'bool']).columns
        for col in categorical_cols:
            if df[col].isnull().sum() > 0:
                mode_value = df[col].mode()[0]
                df[col].fillna(mode_value, inplace=True)
                print(f"✓ {col}: filled with mode")

        print(f"✓ Missing values after imputation: {df.isnull().sum().sum()}")
        return df

    def feature_engineering(self, df):
        """Membuat fitur baru"""
        print("\n--- Feature Engineering ---")
        df = df.copy()

        # Book Age
        df['book_age'] = 2024 - df['publicationYear']
        print("✓ Created: book_age")

        # Has Awards
        if 'awards' in df.columns:
            df['has_awards'] = df['awards'].notna().astype(int)
            print("✓ Created: has_awards")

        # Title Length
        if 'title' in df.columns:
            df['title_length'] = df['title'].str.len()
            print("✓ Created: title_length")

        # Description Length
        if 'description' in df.columns:
            df['description_length'] = df['description'].str.len()
            df['description_length'].fillna(0, inplace=True)
            print("✓ Created: description_length")

        # Tags Count
        if 'tags' in df.columns:
            df['tags_count'] = df['tags'].str.count(',') + 1
            df['tags_count'].fillna(0, inplace=True)
            print("✓ Created: tags_count")

        # Rating Category
        if 'rating_average' in df.columns:
            df['rating_category'] = pd.cut(
                df['rating_average'],
                bins=[0, 3.5, 4.0, 4.5, 5.0],
                labels=['Low', 'Medium', 'High', 'Very High']
            )
            print("✓ Created: rating_category")

        return df

    def encode_categorical(self, df):
        """Encoding variabel kategorikal"""
        print("\n--- Encoding Categorical Variables ---")
        df = df.copy()

        # Convert target ke int
        df['bestseller_status'] = df['bestseller_status'].astype(int)

        # Label Encoding
        categorical_features = ['genre', 'language', 'publisher',
                               'most_popular_country', 'age_category', 'rating_category']

        for col in categorical_features:
            if col in df.columns:
                le = LabelEncoder()
                df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))
                self.label_encoders[col] = le
                print(f"✓ Encoded: {col} → {col}_encoded")

        # Binary encoding
        if 'adapted_to_movie' in df.columns:
            df['adapted_to_movie'] = df['adapted_to_movie'].astype(int)
            print("✓ Converted: adapted_to_movie to binary")

        return df

    def select_features(self, df):
        """Seleksi fitur untuk modeling"""
        print("\n--- Feature Selection ---")

        # Fitur utama
        feature_columns = [
            'rating_average', 'pageCount', 'publicationYear', 'book_age',
            'has_awards', 'title_length', 'adapted_to_movie',
            'genre_encoded', 'language_encoded', 'age_category_encoded'
        ]

        # Fitur opsional
        optional_features = ['description_length', 'tags_count',
                            'publisher_encoded', 'most_popular_country_encoded',
                            'rating_category_encoded']

        for feat in optional_features:
            if feat in df.columns:
                feature_columns.append(feat)

        self.feature_columns = [col for col in feature_columns if col in df.columns]
        print(f"✓ Selected {len(self.feature_columns)} features")

        return df[self.feature_columns + ['bestseller_status']]

    def handle_outliers(self, df):
        """Handle outliers menggunakan IQR method"""
        print("\n--- Handling Outliers (IQR Method) ---")
        df = df.copy()

        numeric_features = ['rating_average', 'pageCount', 'book_age']
        for col in numeric_features:
            if col in df.columns:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR

                outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
                df[col] = df[col].clip(lower_bound, upper_bound)
                print(f"✓ {col}: capped {outliers} outliers")

        return df

    def scale_features(self, X_train, X_test=None):
        """Scaling fitur menggunakan StandardScaler"""
        print("\n--- Feature Scaling ---")

        X_train_scaled = self.scaler.fit_transform(X_train)
        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
        print(f"✓ Scaled {X_train.shape[1]} features")

        if X_test is not None:
            X_test_scaled = self.scaler.transform(X_test)
            X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)
            return X_train_scaled, X_test_scaled

        return X_train_scaled

    def split_data(self, df):
        """Split data menjadi training dan testing"""
        print("\n--- Train-Test Split ---")

        X = df.drop('bestseller_status', axis=1)
        y = df['bestseller_status']

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y
        )

        print(f"✓ Training set: {X_train.shape}")
        print(f"✓ Testing set: {X_test.shape}")
        print(f"✓ Class distribution (train): {y_train.value_counts().to_dict()}")

        return X_train, X_test, y_train, y_test

    def save_preprocessed_data(self, df, X_train, X_test, y_train, y_test, output_dir='preprocessing'):
        """Simpan data yang sudah diproses"""
        print("\n--- Saving Preprocessed Data ---")

        # Buat folder jika belum ada
        os.makedirs(output_dir, exist_ok=True)

        # Save full preprocessed data
        df.to_csv(f'{output_dir}/top_1000_books_preprocessed.csv', index=False)
        print(f"✓ Saved: {output_dir}/top_1000_books_preprocessed.csv")

        # Save train-test split
        train_data = pd.concat([X_train, y_train], axis=1)
        test_data = pd.concat([X_test, y_test], axis=1)

        train_data.to_csv(f'{output_dir}/train_data.csv', index=False)
        test_data.to_csv(f'{output_dir}/test_data.csv', index=False)
        print(f"✓ Saved: {output_dir}/train_data.csv")
        print(f"✓ Saved: {output_dir}/test_data.csv")

        # Save encoders dan scaler
        joblib.dump(self.label_encoders, f'{output_dir}/label_encoders.pkl')
        joblib.dump(self.scaler, f'{output_dir}/scaler.pkl')
        joblib.dump(self.feature_columns, f'{output_dir}/feature_columns.pkl')
        print(f"✓ Saved: encoders and scaler")

    def preprocess(self, filepath, output_dir='preprocessing'):
        """
        Main function untuk menjalankan seluruh pipeline preprocessing

        Parameters:
        -----------
        filepath : str
            Path ke file dataset raw
        output_dir : str
            Directory untuk menyimpan hasil preprocessing

        Returns:
        --------
        X_train, X_test, y_train, y_test : pandas DataFrame/Series
            Data yang sudah diproses dan di-split
        """
        print("="*80)
        print("AUTOMATED PREPROCESSING PIPELINE")
        print("="*80)

        # Load data
        df = self.load_data(filepath)

        # Preprocessing steps
        df = self.handle_missing_values(df)
        df = self.feature_engineering(df)
        df = self.encode_categorical(df)
        df = self.select_features(df)
        df = self.handle_outliers(df)

        # Train-test split
        X_train, X_test, y_train, y_test = self.split_data(df)

        # Scaling
        X_train, X_test = self.scale_features(X_train, X_test)

        # Save results
        df_final = pd.concat([X_train, y_train], axis=1)
        self.save_preprocessed_data(df_final, X_train, X_test, y_train, y_test, output_dir)

        print("\n" + "="*80)
        print("PREPROCESSING COMPLETED SUCCESSFULLY!")
        print("="*80)
        print(f"✓ Features: {len(self.feature_columns)}")
        print(f"✓ Training samples: {X_train.shape[0]}")
        print(f"✓ Testing samples: {X_test.shape[0]}")

        return X_train, X_test, y_train, y_test


# ============================================================================
# MAIN EXECUTION
# ============================================================================
if __name__ == "__main__":
    # Inisialisasi preprocessor
    preprocessor = BookPopularityPreprocessor(
        missing_threshold=0.5,
        test_size=0.2,
        random_state=42
    )

    # Jalankan preprocessing
    X_train, X_test, y_train, y_test = preprocessor.preprocess(
        filepath='top_1000_most_swapped_books_raw.csv',
        output_dir='preprocessing'
    )

    print("\n✓ Preprocessing script executed successfully!")
    print("✓ Check 'preprocessing/' folder for output files")